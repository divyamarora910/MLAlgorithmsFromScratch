{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "data=pd.read_csv(r\"C:\\Users\\divyam.arora\\Desktop\\New Folder 3\\machine-learning-specialization-master\\course-3\\datasets\\amazon_baby_subset.csv\",sep=',')\n",
    "import string\n",
    "data['review'].fillna(value='',inplace=True)\n",
    "data['review']=data['review'].astype(str)\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('','',string.punctuation)) \n",
    "\n",
    "data['review_clean']=data['review'].apply(remove_punctuation)\n",
    "important_words=[\"baby\", \"one\", \"great\", \"love\", \"use\", \"would\", \"like\", \"easy\", \"little\", \"seat\", \"old\", \"well\", \"get\", \"also\", \"really\", \"son\", \"time\", \"bought\", \"product\", \"good\", \"daughter\", \"much\", \"loves\", \"stroller\", \"put\", \"months\", \"car\", \"still\", \"back\", \"used\", \"recommend\", \"first\", \"even\", \"perfect\", \"nice\", \"bag\", \"two\", \"using\", \"got\", \"fit\", \"around\", \"diaper\", \"enough\", \"month\", \"price\", \"go\", \"could\", \"soft\", \"since\", \"buy\", \"room\", \"works\", \"made\", \"child\", \"keep\", \"size\", \"small\", \"need\", \"year\", \"big\", \"make\", \"take\", \"easily\", \"think\", \"crib\", \"clean\", \"way\", \"quality\", \"thing\", \"better\", \"without\", \"set\", \"new\", \"every\", \"cute\", \"best\", \"bottles\", \"work\", \"purchased\", \"right\", \"lot\", \"side\", \"happy\", \"comfortable\", \"toy\", \"able\", \"kids\", \"bit\", \"night\", \"long\", \"fits\", \"see\", \"us\", \"another\", \"play\", \"day\", \"money\", \"monitor\", \"tried\", \"thought\", \"never\", \"item\", \"hard\", \"plastic\", \"however\", \"disappointed\", \"reviews\", \"something\", \"going\", \"pump\", \"bottle\", \"cup\", \"waste\", \"return\", \"amazon\", \"different\", \"top\", \"want\", \"problem\", \"know\", \"water\", \"try\", \"received\", \"sure\", \"times\", \"chair\", \"find\", \"hold\", \"gate\", \"open\", \"bottom\", \"away\", \"actually\", \"cheap\", \"worked\", \"getting\", \"ordered\", \"came\", \"milk\", \"bad\", \"part\", \"worth\", \"found\", \"cover\", \"many\", \"design\", \"looking\", \"weeks\", \"say\", \"wanted\", \"look\", \"place\", \"purchase\", \"looks\", \"second\", \"piece\", \"box\", \"pretty\", \"trying\", \"difficult\", \"together\", \"though\", \"give\", \"started\", \"anything\", \"last\", \"company\", \"come\", \"returned\", \"maybe\", \"took\", \"broke\", \"makes\", \"stay\", \"instead\", \"idea\", \"head\", \"said\", \"less\", \"went\", \"working\", \"high\", \"unit\", \"seems\", \"picture\", \"completely\", \"wish\", \"buying\", \"babies\", \"won\", \"tub\", \"almost\", \"either\"]\n",
    "for word in important_words:\n",
    "    data[word] = data['review_clean'].apply(lambda s : s.split().count(word))\n",
    "features=data.columns.tolist()\n",
    "features.remove('sentiment')\n",
    "features.remove('name')\n",
    "features.remove('review')\n",
    "features.remove('rating')\n",
    "features.remove('review_clean')\n",
    "label=data['sentiment']\n",
    "features=['constant'] + features\n",
    "data['constant'] = 1\n",
    "train_data, validation_data,train_label,validation_label=train_test_split(data[features].as_matrix(),label.as_matrix(),test_size=0.2,random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#link function for logistic regression\n",
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients  \n",
    "    # YOUR CODE HERE\n",
    "    score=np.dot(feature_matrix,coefficients)\n",
    "    \n",
    "    # Compute P(y_i = +1 | x_i, w) using the link function\n",
    "    # YOUR CODE HERE\n",
    "    predictions = 1/(1+np.exp(-score))\n",
    "    \n",
    "    # return predictions\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):     \n",
    "    # Compute the dot product of errors and feature\n",
    "    value=np.dot(errors,feature)\n",
    "    derivative = np.sum(value)\n",
    "    \n",
    "    # Return the derivative\n",
    "    return derivative\n",
    "def feature_derivative_with_L2(errors, feature, coefficient, l2_penalty, feature_is_constant): \n",
    "    \n",
    "    # Compute the dot product of errors and feature\n",
    "    value=np.dot(errors,feature)\n",
    "    derivative = np.sum(value)\n",
    "\n",
    "    # add L2 penalty term for any feature that isn't the intercept.\n",
    "    if not feature_is_constant: \n",
    "        ## YOUR CODE HERE\n",
    "        derivative=derivative-(2*(l2_penalty*coefficient))\n",
    "        \n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    lp = np.sum((indicator-1)*scores - np.log(1. + np.exp(-scores))) - l2_penalty*np.sum(coefficients[1:]**2)\n",
    "    \n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "from math import sqrt\n",
    "\n",
    "def logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    for itr in range(max_iter):\n",
    "        # Predict P(y_i = +1|x_i,w) using your predict_probability() function\n",
    "        ## YOUR CODE HERE\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "        \n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment==+1)\n",
    "        \n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "        for j in range(len(coefficients)): # loop over each coefficient\n",
    "            is_intercept = (j == 0)\n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j].\n",
    "            # Compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            ## YOUR CODE HERE\n",
    "            derivative = feature_derivative(errors, feature_matrix[:,j])\n",
    "            \n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            ## YOUR CODE HERE\n",
    "            coefficients[j]=coefficients[j]+(derivative*step_size)\n",
    "        \n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty)\n",
    "            print('iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp))\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29244.41002597\n",
      "iteration   1: log likelihood of observed labels = -29067.83207373\n",
      "iteration   2: log likelihood of observed labels = -28898.25174925\n",
      "iteration   3: log likelihood of observed labels = -28734.98531155\n",
      "iteration   4: log likelihood of observed labels = -28577.52232358\n",
      "iteration   5: log likelihood of observed labels = -28425.46142638\n",
      "iteration   6: log likelihood of observed labels = -28278.47182690\n",
      "iteration   7: log likelihood of observed labels = -28136.27006088\n",
      "iteration   8: log likelihood of observed labels = -27998.60582965\n",
      "iteration   9: log likelihood of observed labels = -27865.25324782\n",
      "iteration  10: log likelihood of observed labels = -27736.00533299\n",
      "iteration  11: log likelihood of observed labels = -27610.67044681\n",
      "iteration  12: log likelihood of observed labels = -27489.06991675\n",
      "iteration  13: log likelihood of observed labels = -27371.03637748\n",
      "iteration  14: log likelihood of observed labels = -27256.41255593\n",
      "iteration  15: log likelihood of observed labels = -27145.05033502\n",
      "iteration  20: log likelihood of observed labels = -26632.53364708\n",
      "iteration  30: log likelihood of observed labels = -25786.63713726\n",
      "iteration  40: log likelihood of observed labels = -25115.66041180\n",
      "iteration  50: log likelihood of observed labels = -24568.80867952\n",
      "iteration  60: log likelihood of observed labels = -24113.31325118\n",
      "iteration  70: log likelihood of observed labels = -23727.13252187\n",
      "iteration  80: log likelihood of observed labels = -23394.89805743\n",
      "iteration  90: log likelihood of observed labels = -23105.55852050\n",
      "iteration 100: log likelihood of observed labels = -22850.95328777\n",
      "iteration 200: log likelihood of observed labels = -21336.78345702\n",
      "iteration 300: log likelihood of observed labels = -20622.96621804\n",
      "iteration 400: log likelihood of observed labels = -20202.99147681\n",
      "iteration 500: log likelihood of observed labels = -19926.33201756\n"
     ]
    }
   ],
   "source": [
    "# run with L2 = 0\n",
    "coefficients_0_penalty = logistic_regression_with_L2(train_data, train_label,\n",
    "                                                     initial_coefficients=np.zeros(194),\n",
    "                                                     step_size=5e-6, l2_penalty=0, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29244.41375958\n",
      "iteration   1: log likelihood of observed labels = -29067.84667170\n",
      "iteration   2: log likelihood of observed labels = -28898.28389643\n",
      "iteration   3: log likelihood of observed labels = -28735.04130516\n",
      "iteration   4: log likelihood of observed labels = -28577.60811457\n",
      "iteration   5: log likelihood of observed labels = -28425.58265020\n",
      "iteration   6: log likelihood of observed labels = -28278.63382776\n",
      "iteration   7: log likelihood of observed labels = -28136.47791168\n",
      "iteration   8: log likelihood of observed labels = -27998.86434911\n",
      "iteration   9: log likelihood of observed labels = -27865.56701570\n",
      "iteration  10: log likelihood of observed labels = -27736.37870381\n",
      "iteration  11: log likelihood of observed labels = -27611.10756253\n",
      "iteration  12: log likelihood of observed labels = -27489.57471850\n",
      "iteration  13: log likelihood of observed labels = -27371.61261652\n",
      "iteration  14: log likelihood of observed labels = -27257.06380396\n",
      "iteration  15: log likelihood of observed labels = -27145.77999379\n",
      "iteration  20: log likelihood of observed labels = -26633.70104275\n",
      "iteration  30: log likelihood of observed labels = -25788.85925620\n",
      "iteration  40: log likelihood of observed labels = -25119.10170304\n",
      "iteration  50: log likelihood of observed labels = -24573.57437254\n",
      "iteration  60: log likelihood of observed labels = -24119.47067068\n",
      "iteration  70: log likelihood of observed labels = -23734.72390621\n",
      "iteration  80: log likelihood of observed labels = -23403.94858027\n",
      "iteration  90: log likelihood of observed labels = -23116.08148691\n",
      "iteration 100: log likelihood of observed labels = -22862.95360107\n",
      "iteration 200: log likelihood of observed labels = -21363.05084309\n",
      "iteration 300: log likelihood of observed labels = -20661.97852513\n",
      "iteration 400: log likelihood of observed labels = -20253.29209116\n",
      "iteration 500: log likelihood of observed labels = -19986.70443806\n"
     ]
    }
   ],
   "source": [
    "# run with L2 = 4\n",
    "coefficients_4_penalty = logistic_regression_with_L2(train_data, train_label,\n",
    "                                                      initial_coefficients=np.zeros(194),\n",
    "                                                      step_size=5e-6, l2_penalty=4, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29244.41936000\n",
      "iteration   1: log likelihood of observed labels = -29067.86856866\n",
      "iteration   2: log likelihood of observed labels = -28898.33211720\n",
      "iteration   3: log likelihood of observed labels = -28735.12529558\n",
      "iteration   4: log likelihood of observed labels = -28577.73680104\n",
      "iteration   5: log likelihood of observed labels = -28425.76448594\n",
      "iteration   6: log likelihood of observed labels = -28278.87682905\n",
      "iteration   7: log likelihood of observed labels = -28136.78968788\n",
      "iteration   8: log likelihood of observed labels = -27999.25212831\n",
      "iteration   9: log likelihood of observed labels = -27866.03766752\n",
      "iteration  10: log likelihood of observed labels = -27736.93876004\n",
      "iteration  11: log likelihood of observed labels = -27611.76323610\n",
      "iteration  12: log likelihood of observed labels = -27490.33192111\n",
      "iteration  13: log likelihood of observed labels = -27372.47697508\n",
      "iteration  14: log likelihood of observed labels = -27258.04067600\n",
      "iteration  15: log likelihood of observed labels = -27146.87448193\n",
      "iteration  20: log likelihood of observed labels = -26635.45213626\n",
      "iteration  30: log likelihood of observed labels = -25792.19243462\n",
      "iteration  40: log likelihood of observed labels = -25124.26363991\n",
      "iteration  50: log likelihood of observed labels = -24580.72291208\n",
      "iteration  60: log likelihood of observed labels = -24128.70679995\n",
      "iteration  70: log likelihood of observed labels = -23746.11098271\n",
      "iteration  80: log likelihood of observed labels = -23417.52436453\n",
      "iteration  90: log likelihood of observed labels = -23131.86593654\n",
      "iteration 100: log likelihood of observed labels = -22880.95407101\n",
      "iteration 200: log likelihood of observed labels = -21402.45192220\n",
      "iteration 300: log likelihood of observed labels = -20720.49698578\n",
      "iteration 400: log likelihood of observed labels = -20328.74301267\n",
      "iteration 500: log likelihood of observed labels = -20077.26306880\n"
     ]
    }
   ],
   "source": [
    "# run with L2 = 10\n",
    "coefficients_10_penalty = logistic_regression_with_L2(train_data, train_label,\n",
    "                                                      initial_coefficients=np.zeros(194),\n",
    "                                                      step_size=5e-6, l2_penalty=10, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29244.50336630\n",
      "iteration   1: log likelihood of observed labels = -29068.19702307\n",
      "iteration   2: log likelihood of observed labels = -28899.05542867\n",
      "iteration   3: log likelihood of observed labels = -28736.38515182\n",
      "iteration   4: log likelihood of observed labels = -28579.66709818\n",
      "iteration   5: log likelihood of observed labels = -28428.49202194\n",
      "iteration   6: log likelihood of observed labels = -28282.52184844\n",
      "iteration   7: log likelihood of observed labels = -28141.46633081\n",
      "iteration   8: log likelihood of observed labels = -28005.06881623\n",
      "iteration   9: log likelihood of observed labels = -27873.09744480\n",
      "iteration  10: log likelihood of observed labels = -27745.33960351\n",
      "iteration  11: log likelihood of observed labels = -27621.59833974\n",
      "iteration  12: log likelihood of observed labels = -27501.68996032\n",
      "iteration  13: log likelihood of observed labels = -27385.44235349\n",
      "iteration  14: log likelihood of observed labels = -27272.69375660\n",
      "iteration  15: log likelihood of observed labels = -27163.29180413\n",
      "iteration  20: log likelihood of observed labels = -26661.71853888\n",
      "iteration  30: log likelihood of observed labels = -25842.19011086\n",
      "iteration  40: log likelihood of observed labels = -25201.69269287\n",
      "iteration  50: log likelihood of observed labels = -24687.95100507\n",
      "iteration  60: log likelihood of observed labels = -24267.24873890\n",
      "iteration  70: log likelihood of observed labels = -23916.91713027\n",
      "iteration  80: log likelihood of observed labels = -23621.16112841\n",
      "iteration  90: log likelihood of observed labels = -23368.63268085\n",
      "iteration 100: log likelihood of observed labels = -23150.96112018\n",
      "iteration 200: log likelihood of observed labels = -21993.46810880\n",
      "iteration 300: log likelihood of observed labels = -21598.27389542\n",
      "iteration 400: log likelihood of observed labels = -21460.50683537\n",
      "iteration 500: log likelihood of observed labels = -21435.64253002\n"
     ]
    }
   ],
   "source": [
    "# run with L2 = 1e2\n",
    "coefficients_1e2_penalty = logistic_regression_with_L2(train_data, train_label,\n",
    "                                                       initial_coefficients=np.zeros(194),\n",
    "                                                       step_size=5e-6, l2_penalty=1e2, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29245.34342928\n",
      "iteration   1: log likelihood of observed labels = -29071.48156711\n",
      "iteration   2: log likelihood of observed labels = -28906.28854345\n",
      "iteration   3: log likelihood of observed labels = -28748.98371419\n",
      "iteration   4: log likelihood of observed labels = -28598.97006955\n",
      "iteration   5: log likelihood of observed labels = -28455.76738196\n",
      "iteration   6: log likelihood of observed labels = -28318.97204229\n",
      "iteration   7: log likelihood of observed labels = -28188.23276018\n",
      "iteration   8: log likelihood of observed labels = -28063.23569550\n",
      "iteration   9: log likelihood of observed labels = -27943.69521761\n",
      "iteration  10: log likelihood of observed labels = -27829.34803820\n",
      "iteration  11: log likelihood of observed labels = -27719.94937608\n",
      "iteration  12: log likelihood of observed labels = -27615.27035242\n",
      "iteration  13: log likelihood of observed labels = -27515.09613751\n",
      "iteration  14: log likelihood of observed labels = -27419.22456263\n",
      "iteration  15: log likelihood of observed labels = -27327.46502604\n",
      "iteration  20: log likelihood of observed labels = -26924.38256512\n",
      "iteration  30: log likelihood of observed labels = -26342.16687327\n",
      "iteration  40: log likelihood of observed labels = -25975.98322250\n",
      "iteration  50: log likelihood of observed labels = -25760.23193501\n",
      "iteration  60: log likelihood of observed labels = -25652.66812837\n",
      "iteration  70: log likelihood of observed labels = -25624.97860586\n",
      "iteration  80: log likelihood of observed labels = -25657.52876725\n",
      "iteration  90: log likelihood of observed labels = -25736.30012403\n",
      "iteration 100: log likelihood of observed labels = -25851.03161185\n",
      "iteration 200: log likelihood of observed labels = -27903.62997481\n",
      "iteration 300: log likelihood of observed labels = -30376.04299186\n",
      "iteration 400: log likelihood of observed labels = -32778.14506240\n",
      "iteration 500: log likelihood of observed labels = -35019.43714213\n"
     ]
    }
   ],
   "source": [
    "# run with L2 = 1e3\n",
    "coefficients_1e3_penalty = logistic_regression_with_L2(train_data, train_label,\n",
    "                                                       initial_coefficients=np.zeros(194),\n",
    "                                                       step_size=5e-6, l2_penalty=1e3, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29337.75035722\n",
      "iteration   1: log likelihood of observed labels = -29432.78141131\n",
      "iteration   2: log likelihood of observed labels = -29701.93116875\n",
      "iteration   3: log likelihood of observed labels = -30134.82557518\n",
      "iteration   4: log likelihood of observed labels = -30722.29692008\n",
      "iteration   5: log likelihood of observed labels = -31456.05698437\n",
      "iteration   6: log likelihood of observed labels = -32328.49336598\n",
      "iteration   7: log likelihood of observed labels = -33332.53999053\n",
      "iteration   8: log likelihood of observed labels = -34461.59241521\n",
      "iteration   9: log likelihood of observed labels = -35709.45022668\n",
      "iteration  10: log likelihood of observed labels = -37070.27585371\n",
      "iteration  11: log likelihood of observed labels = -38538.56337355\n",
      "iteration  12: log likelihood of observed labels = -40109.11348365\n",
      "iteration  13: log likelihood of observed labels = -41777.01238028\n",
      "iteration  14: log likelihood of observed labels = -43537.61322536\n",
      "iteration  15: log likelihood of observed labels = -45386.51943625\n",
      "iteration  20: log likelihood of observed labels = -55817.42545122\n",
      "iteration  30: log likelihood of observed labels = -81339.61073825\n",
      "iteration  40: log likelihood of observed labels = -111147.94148224\n",
      "iteration  50: log likelihood of observed labels = -143711.13422832\n",
      "iteration  60: log likelihood of observed labels = -178048.80097094\n",
      "iteration  70: log likelihood of observed labels = -213511.74092068\n",
      "iteration  80: log likelihood of observed labels = -249657.96903958\n",
      "iteration  90: log likelihood of observed labels = -286179.71887329\n",
      "iteration 100: log likelihood of observed labels = -322858.78569508\n",
      "iteration 200: log likelihood of observed labels = -678021.43523574\n",
      "iteration 300: log likelihood of observed labels = -995930.64359999\n",
      "iteration 400: log likelihood of observed labels = -1277718.35003500\n",
      "iteration 500: log likelihood of observed labels = -1529236.84447489\n"
     ]
    }
   ],
   "source": [
    "# run with L2 = 1e5\n",
    "coefficients_1e5_penalty = logistic_regression_with_L2(train_data, train_label,\n",
    "                                                       initial_coefficients=np.zeros(194),\n",
    "                                                       step_size=5e-6, l2_penalty=1e5, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = [str(i) for i in range(1, len(coefficients_1e5_penalty)+1)]\n",
    "coefficients_1e5_penalty_df = pd.DataFrame(coefficients_1e5_penalty, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = [str(i) for i in range(1, len(coefficients_1e3_penalty)+1)]\n",
    "coefficients_1e3_penalty_df = pd.DataFrame(coefficients_1e3_penalty, index=index)\n",
    "index = [str(i) for i in range(1, len(coefficients_1e2_penalty)+1)]\n",
    "coefficients_1e2_penalty_df = pd.DataFrame(coefficients_1e2_penalty, index=index)\n",
    "index = [str(i) for i in range(1, len(coefficients_10_penalty)+1)]\n",
    "coefficients_10_penalty_df = pd.DataFrame(coefficients_10_penalty, index=index)\n",
    "index = [str(i) for i in range(1, len(coefficients_4_penalty)+1)]\n",
    "coefficients_4_penalty_df = pd.DataFrame(coefficients_4_penalty, index=index)\n",
    "index = [str(i) for i in range(1, len(coefficients_0_penalty)+1)]\n",
    "coefficients_0_penalty_df = pd.DataFrame(coefficients_0_penalty, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefficients_0_penalty_df = pd.concat([coefficients_0_penalty_df, coefficients_4_penalty_df,coefficients_10_penalty_df,coefficients_1e2_penalty_df,coefficients_1e3_penalty_df,coefficients_1e5_penalty_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0777747 ,  0.07440832,  0.01138698,  0.79318383,  1.03485777,\n",
       "        0.00456272, -0.30465027, -0.01789266,  0.9793676 ,  0.54279474,\n",
       "       -0.09943876,  0.19430276,  0.46459339, -0.17994936,  0.16125216,\n",
       "        0.00388761,  0.15072199, -0.0722893 , -0.12615965, -0.26264138,\n",
       "        0.16869205,  0.27904503, -0.00369078,  1.05179425, -0.03365981,\n",
       "       -0.0029959 , -0.07043067,  0.19484338,  0.19774942, -0.26457855,\n",
       "        0.07938766,  0.40986771, -0.0821891 , -0.388606  ,  0.85729344,\n",
       "        0.44552773,  0.00320934, -0.09710669,  0.04493217, -0.09846741,\n",
       "       -0.15442385,  0.09238314,  0.07004575,  0.03925715, -0.16706904,\n",
       "        0.27884193,  0.04062695, -0.16973209,  0.34527228,  0.07200049,\n",
       "       -0.28326893,  0.2773654 ,  0.32105061, -0.13841674, -0.06083573,\n",
       "        0.17122948,  0.18688569, -0.16858369,  0.22570894,  0.08131243,\n",
       "        0.02827011, -0.03212197,  0.15027508,  0.12103769,  0.08587272,\n",
       "        0.10440364,  0.11550759, -0.30030678, -0.06822406, -0.20747563,\n",
       "       -0.21608915,  0.26951544,  0.16602704,  0.00929533,  0.17234091,\n",
       "        0.07019984,  0.53320295,  0.06677023, -0.53186412, -0.07788142,\n",
       "       -0.0887564 ,  0.23767687,  0.00836033,  0.57962821,  0.34490498,\n",
       "       -0.05535139,  0.15517693,  0.25359566,  0.32576831,  0.22219768,\n",
       "        0.07952496,  0.4734668 , -0.06142249,  0.09721583, -0.1791029 ,\n",
       "        0.19124293,  0.0739228 , -0.75763356, -0.20501462, -0.33350644,\n",
       "       -0.47340016, -0.21469321, -0.30359178, -0.33465841, -0.27303824,\n",
       "       -0.23103369, -0.95611158, -0.2471723 , -0.3069756 , -0.17455108,\n",
       "        0.10774257, -0.05090708, -0.19610667, -0.61125333, -0.74272778,\n",
       "       -0.05946075, -0.07320469, -0.10689502, -0.03757367, -0.08909321,\n",
       "       -0.01128219, -0.0509511 , -0.18468825, -0.28603122,  0.03031105,\n",
       "       -0.16552721, -0.07905576, -0.02683376, -0.02784743,  0.00433689,\n",
       "       -0.23356531, -0.16043841, -0.17664912, -0.12328639, -0.43840343,\n",
       "       -0.22574582, -0.00462219, -0.10880259, -0.09380276, -0.09977976,\n",
       "       -0.37052967, -0.16826272,  0.02306003,  0.05226166, -0.02178735,\n",
       "        0.09794098, -0.25961993,  0.01263013, -0.14761243, -0.03268689,\n",
       "       -0.06902434, -0.10135492, -0.06031878,  0.05887073, -0.08273018,\n",
       "        0.02927351, -0.22497839, -0.23296068,  0.05246016, -0.2654596 ,\n",
       "       -0.34565781, -0.08111541,  0.14166447,  0.00710964, -0.13853567,\n",
       "       -0.15574425, -0.11695297, -0.25140442, -0.03611463, -0.58291681,\n",
       "       -0.19078988, -0.02647499, -0.5412405 , -0.00313998, -0.27473179,\n",
       "       -0.18245406, -0.4776882 , -0.07934374, -0.07011162, -0.10547809,\n",
       "       -0.08747283, -0.31976218, -0.02684492, -0.18954815,  0.04925553,\n",
       "       -0.2009555 , -0.26408512,  0.19499956, -0.11521517,  0.04302149,\n",
       "        0.00120782, -0.17685036, -0.06338812, -0.23330741])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients_4_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
